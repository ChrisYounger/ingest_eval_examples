# This sourcetype is for the conflicting timestamps usecase, it calls a transform to extract the 
# datetime format via INGEST_EVAL and the strptime function
[conflicting_datetime_formats]
DATETIME_CONFIG = CURRENT
TRANSFORMS-extract_date = conflicting_datetime_formats-test_try_formats

# This sourcetype is for the compound datetimes examples where the file name encodes the date stamp
# but the timestamp is per log line
[compound_datetimes]
DATETIME_CONFIG = CURRENT
TRANSFORMS-get-date = compound_datetimes-join_and_parse
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)

# This configuration is applied to all sourcetypes and it adds an indexed field with len of the event
# This is very useful for using with tstats to sum up all ingested data from any source very quickly
[default]
TRANSFORM-all-data= add_event_length-get_raw_length

# this is an example for removing unwanted indexed fields from a CSV file
[drop_indexed_fields]
# The timestamp is found in the time column, we need to remove this column
TIMESTAMP_FIELDS=time
INDEXED_EXTRACTIONS = CSV
# These transforms remove the useless columns, and also the useless time fields
TRANSFORMS-drop_fields = drop_indexed_fields-drop_useless_columns, drop_indexed_fields-drop_useless_time_fields
# Lift the removed columns via search time exraction (only works in the app)
EXTRACT-removed-columns= [^,]+,[^,]+,[^,]+,(?<random_nonsense>[^,]+),(?<long_payload>[^,]+)

# this is an example for importing data from an external splunk instance
[load_into_indexes]
# time is stored in epoch at the start of each line
TIME_FORMAT = %s.%3Q
TIME_PREFIX = ^
# the transform required to drop the header extract the metafields and copy to the correct fields
TRANSFORMS-extract-metadata = load_into_indexes-drop_header, load_into_indexes-extract_metadata_copy_to_meta, load_into_indexes-reassign_meta_to_metadata, load_into_indexes-remove_metadata_from_raw
# Splunk uses double quotes to escape quotes, we want to remove this before we start extracting the fields
SEDCMD-strip_double_quotes= s/""/"/g
# The solution supports multiline events
LINE_BREAKER=(\^\^\^END\^\^\^"\n)
SHOULD_LINEMERGE=false

# This sourcetype is an example for how we can use REPEAT_MATCH and regex to automatically extract fields from log files
[auto_extract_indexed_fields]
TIME_PREFIX = ^
TIME_FORMAT = %Y-%m-%d %H:%M:%S
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
# because we are creating indexed fields we can disable the major breakers
SEGMENTATION = search
TRANSFORMS-extract_indexed_fields=auto_extract_indexed_fields-doubled_quoted_av_pairs, auto_extract_indexed_fields-single_quoted_av_pairs, auto_extract_indexed_fields-unquoted_av_pairs

[advanced_masking]
TIME_PREFIX = ^
TIME_FORMAT = %Y-%m-%d %H:%M:%S
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
TRANSFORMS-apply-masking=extract_email


[split_output]
TIME_PREFIX = ^
TIME_FORMAT = %Y-%m-%d %H:%M:%S
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
