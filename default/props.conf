# This sourcetype is for the conflicting timestamps usecase, it calls a transform to extract the 
# datetime format via INGEST_EVAL and the strptime function
[conflicting_datetime_formats]
DATETIME_CONFIG = CURRENT
TRANSFORMS-extract_date = conflicting_datetime_formats-test_try_formats

# This sourcetype is for the compound datetimes examples where the file name encodes the date stamp
# but the timestamp is per log line
[compound_datetimes]
DATETIME_CONFIG = CURRENT
TRANSFORMS-get-date = compound_datetimes-join_and_parse
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)

# This configuration is applied to all sourcetypes and it adds an indexed field with len of the event
# This is very useful for using with tstats to sum up all ingested data from any source very quickly
[default]
TRANSFORM-all-data= add_event_length-get_raw_length

# this is an example for removing unwanted indexed fields from a CSV file
[drop_indexed_fields]
# The timestamp is found in the time column, we need to remove this column
TIMESTAMP_FIELDS=time
INDEXED_EXTRACTIONS = CSV
# These transforms remove the useless columns, and also the useless time fields
TRANSFORMS-drop_fields = drop_indexed_fields-drop_useless_columns, shared-drop_useless_time_fields
# Lift the removed columns via search time exraction (only works in the app)
EXTRACT-removed-columns= [^,]+,[^,]+,[^,]+,(?<random_nonsense>[^,]+),(?<long_payload>[^,]+)

# this is an example for importing data from an external splunk instance
[load_into_indexes]
# time is stored in epoch at the start of each line
TIME_FORMAT = %s.%3Q
TIME_PREFIX = ^
# the transform required to drop the header extract the metafields and copy to the correct fields
TRANSFORMS-extract-metadata = load_into_indexes-drop_header, load_into_indexes-extract_metadata_copy_to_meta, load_into_indexes-reassign_meta_to_metadata, load_into_indexes-remove_metadata_from_raw
# Splunk uses double quotes to escape quotes, we want to remove this before we start extracting the fields
SEDCMD-strip_double_quotes= s/""/"/g
# The solution supports multiline events
LINE_BREAKER=(\^\^\^END\^\^\^"\n)
SHOULD_LINEMERGE=false

# This sourcetype is an example for how we can use REPEAT_MATCH and regex to automatically extract fields from log files
[auto_extract_indexed_fields]
TIME_PREFIX = ^
TIME_FORMAT = %Y-%m-%d %H:%M:%S
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
# because we are creating indexed fields we can disable the major breakers
SEGMENTATION = search
TRANSFORMS-extract_indexed_fields=auto_extract_indexed_fields-univeral

[enrich_splunkd_component_level_thread]
TIME_PREFIX = ^
TIME_FORMAT = %m-%d-%Y %H:%M:%S.%l %z
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
TRANSFORMS-enrich-splunkd-log_level = enrich_splunkd_component_level_thread-extract_log_level_etc, enrich_splunkd_component_level_thread-drop_null_thread_info

[splunkd]
TRANSFORMS-enrich-splunkd-log_level = auto_extract_indexed_fields-univeral, enrich_splunkd_component_level_thread-extract_log_level_etc, enrich_splunkd_component_level_thread-drop_null_thread_info

[advanced_masking]
TIME_PREFIX = ^
TIME_FORMAT = %Y-%m-%d %H:%M:%S
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
TRANSFORMS-apply-masking=extract_email

# This sourcetype sends data out via TCP out
[split_forwarding]
TIME_PREFIX = ^
TIME_FORMAT = %Y-%m-%d %H:%M:%S
SHOULD_LINEMERGE=false
LINE_BREAKER=([\n\r]+)
rename = split_forwarding_sendout
TRANSFORMS-split_forwarding=split_forwarding-randomize_output

# license usage data is under the splunkd sourcetype, so we need to match on the source file to be more selective 
# The transform uses CLONE_SOURCETYPE to create a copy of the event for further processing
[source::.../var/log/splunk/license_usage.log(.\d+)?]
TRANSFORMS-clone_license = metricify_license-clone

# These transforms convert the event into a metrics compliant one, and steer to the _metrics index 
[metricify_license]
TRANSFORMS-clone_and_convert = metricify_license-extract_fields, shared-drop_useless_time_fields, metricify_license-reassign_meta_fields_and_route_to_index