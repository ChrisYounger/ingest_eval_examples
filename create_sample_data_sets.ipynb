{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook generates sample data for the app and writes to the sample directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the global variables for the script\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "\"\"\" \n",
    "For events we have selected a selection of Splunk T shirt sloans. This list was obtained by searching the web, it is not a definitive list and I suspect many were never printed :-)\n",
    "\"\"\"\n",
    "\n",
    "log_lines=[\"splunk> Finding your faults, just like mom.\", \n",
    "    \"splunk> because ninjas are too busy\", \n",
    "    \"splunk> All batbelt. No tights.\", \n",
    "    \"splunk> Digs deeper than a jealous spouse.\", \n",
    "    \"splunk> More flexible than an Olympic gymnast.\", \n",
    "    \"splunk> The mars rover of the IT landfill.\", \n",
    "    \"Splunk> The IT Search Engine.\", \n",
    "    \"Splunk> Be an IT superhero. Go home early.\", \n",
    "    \"Splunk> CSI: Logfiles.\", \n",
    "    \"Splunk> Needle. Haystack. Found.\", \n",
    "    \"Splunk> All batbelt. No tights.\", \n",
    "    \"Splunk> Finding your faults, just like mom.\", \n",
    "    \"Splunk> Australian for grep.\", \n",
    "    \"Splunk> 4TW\", \n",
    "    \"Splunk> See your world. Maybe wish you hadnâ€™t.\", \n",
    "    \"Splunk> Like an F-18, bro.\", \n",
    "    \"Splunk> Now with more code!\", \n",
    "    \"Splunk> Winning the War on Error\", \n",
    "    \"Splunk> The Notorious B.I.G. D.A.T.A.\", \n",
    "    \"Splunk> Map. Reduce. Recycle.\", \n",
    "    \"Splunk> Take the sh out of IT.\", \n",
    "    \"Splunk> I like big data and I cannot lie.\", \n",
    "    \"splunk> I gotta fever, and the only cure is MOAR LICENSE!\", \n",
    "    \"splunk> The corkscrew for your vintage data.\", \n",
    "    \"splunk> Caught me on the server - Wasn't me.\", \n",
    "    \"splunk> \\\"\\\"\\. nuff said.\", \n",
    "    \"splunk> These are the droids you are looking for\", \n",
    "    \"splunk> Finding disturbances in the Force before the Jedi Masters\", \n",
    "    \"splunk> don't get caught up in the game of pwns\", \n",
    "    \"splunk> We enjoy breaks more than Unions\", \n",
    "    \"splunk> We line break for regular expressions\", \n",
    "    \"splunk> The bran for your system\", \n",
    "    \"splunk> Open a can of whooparse\", \n",
    "    \"splunk> Show me your logs\", \n",
    "    \"splunk> Rhymes with drunk\", \n",
    "    \"splunk> Chasing tail since 2003\", \n",
    "    \"splunk> this way: Run-D.M.C.\", \n",
    "    \"splunk> Walking War Room!!\", \n",
    "    \"splunk> IT like you mean it\", \n",
    "    \"splunk ML> Solve problems you didn't know you were about to have\", \n",
    "    \"Splunk> see the forest, and the trees\", \n",
    "    \"Splunk> data with destiny\", \n",
    "    \"Splunk> see the light before you tunnel\"]\n",
    "\n",
    "\"\"\"\n",
    "The script generates events randomly over a time range, by default this goes back 5 days and generates a 1000 events each time.\n",
    "\"\"\"\n",
    "date_range_days=5\n",
    "sample_readings=1000\n",
    "seconds_in_day=24*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This generates a list of events where the time stamps switches between 3 different timestamps.\n",
    "\n",
    "This is a common problem in badly designed Splunk instances. People open a TCP port and then fire all sorts of different data in there. \n",
    "\n",
    "Ideally we would create multiple sourcetypes and then assign a TCP port for each sourcetype. However the example shows you how to patch the problem during ingestion.\n",
    "\"\"\"\n",
    "\n",
    "# Firstly we will create a list of times for the events\n",
    "datetimes =[]\n",
    "for i in range(1,sample_readings) :\n",
    "    random_seconds = random.randrange(1, date_range_days*seconds_in_day)\n",
    "    datetimes.append(datetime.timedelta(seconds=-random_seconds))\n",
    "\n",
    "# We want the events to be sorted from the earliest to the latest\n",
    "# this isn't strickly necessary, but more realistic.\n",
    "datetimes.sort(reverse=True)\n",
    "\n",
    "# our three different date time formats\n",
    "datetime_format = [\"%Y-%m-%d %H:%M:%S\", \"%H:%M:%S %y-%m-%d\", \"%c\"]\n",
    "\n",
    "# create out output file\n",
    "mutliplexed_datetime_formats = open(\"sample/conflicting_dates/mutliplexed_datetime_formats.log\",\"w\")\n",
    "\n",
    "# iterate through the list of date timesn and write out to disk\n",
    "for i in datetimes :\n",
    "    # select a timeformat at random and use it\n",
    "    time=(datetime.datetime.now()-i).strftime(random.choice(datetime_format))\n",
    "    # pick a random log line to use\n",
    "    message=random.choice(log_lines)\n",
    "    # write out the log file\n",
    "    mutliplexed_datetime_formats.write(time+\" \"+message+\"\\n\")\n",
    "\n",
    "# close and flush the file\n",
    "mutliplexed_datetime_formats.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script generates events where the date is embedded in the file name, but the timestamp is per event im the contents of the file.\n",
    "\n",
    "We are going to create a map of dates to times, so that we can itterate through each day, create a file and fill with events for that day\n",
    "\n",
    "To work around any weird rounding errors due to timezones we will generate the day, and the seconds separately\n",
    "\"\"\"\n",
    "\n",
    "# a map of dates to timings\n",
    "date_map = {}\n",
    "\n",
    "for i in range(1,sample_readings) :\n",
    "    # lets pick a random day back in time\n",
    "    random_day = random.randrange(0,date_range_days)\n",
    "    # lets pick the number of seconds into that day\n",
    "    random_seconds = random.randrange(1, seconds_in_day)\n",
    "    # enter the time stamp into the map\n",
    "    if random_day not in date_map :\n",
    "        # we need to create a new entry into the map\n",
    "        date_map[random_day] = [datetime.timedelta(days=-random_day, seconds=-random_seconds)]\n",
    "    else:\n",
    "        # the day already exists, lets append this new date time\n",
    "        date_map[random_day].append(datetime.timedelta(days=-random_day, seconds=-random_seconds))\n",
    "\n",
    "# itterate through the list of days in the map    \n",
    "for i in date_map.keys() :\n",
    "    # lets create the filename for the days events, named after the day \"2020-02-12.log\"\n",
    "    filename=\"sample/compound_date_time/\"+(datetime.datetime.now()-datetime.timedelta(days=-i)).strftime(\"%Y-%m-%d\")+\".log\"\n",
    "    # create the file\n",
    "    file_for_day = open(filename,\"w\")\n",
    "    # sort the dates into cronological order\n",
    "    date_map[i].sort(reverse=True)\n",
    "    # for each timestamp in the day create a log message\n",
    "    for t in date_map[i] :\n",
    "        # create the timestamp with hours and days only \n",
    "        time = (datetime.datetime.now()-t).strftime(\"%H:%M:%S\")\n",
    "        # write out the timestamp with a random log message\n",
    "        file_for_day.write(time+\" \"+random.choice(log_lines)+\"\\n\")\n",
    "    # close the file and move on to the next day\n",
    "    file_for_day.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script generates a csv with 'useless' columns that we don't want to add into tsidx because they will bloat the size of the bucket.\n",
    "\n",
    "We use pandas to build the CSV file, set headers etc\n",
    "\"\"\"\n",
    "\n",
    "# create a pandas with some column headings describing the contents\n",
    "useless_columns=pandas.DataFrame(columns=['primary_key', 'primary_value', 'repeated_field', 'random_nonsense', 'long_payload'])\n",
    "\n",
    "# Create rows and assign values to the columns\n",
    "for i in range(0,sample_readings) :\n",
    "    useless_columns=useless_columns.append({'primary_key': i, 'primary_value': random.randint(0,999999), 'repeated_field': \"same silly value\", 'random_nonsense' : uuid.uuid4(), 'long_payload' : random.choice(log_lines)}, ignore_index=True)\n",
    "\n",
    "# write out the CSV file\n",
    "useless_columns.to_csv('sample/drop_useless_columns/useless_columns.csv', sep=',', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script generates a data set for importing into directly into splunk. We have create sourcetype, source, host, index and then use INGEST_EVAL + REGEX to extract the fields and copy them into the relevant fields. \n",
    "\n",
    "The format aims to replicated the output of the following splunk search:\n",
    "\"\"\"\n",
    "\n",
    "indexes=['ingest_bob', 'ingest_tom', 'ingest_buttercup']\n",
    "sourcetypes=[('ingest_bananas', \"%c\"), ('ingest_meat', \"%Y-%m-%d %H:%M:%S\"), ('ingest_pairs', \"%H:%M:%S %y-%m-%d\"), ('ingest_apples', \"%d %a %Y %H:%M:%S\")]\n",
    "sources=['sea', 'ground', 'sky', 'tree']\n",
    "hosts=['server', 'laptop', 'phone']\n",
    "\n",
    "import_events = open('sample/import_data/encoded_splunk_events.txt',\"w\")\n",
    "\n",
    "mutliplexed_datetime_formats = open(\"sample/conflicting_dates/mutliplexed_datetime_formats.log\",\"w\")\n",
    "sep=\"%%%\"\n",
    "\n",
    "for i in datetimes :\n",
    "    (sourcetype, datetime_format) = random.choice(sourcetypes)\n",
    "    time=str((datetime.datetime.now()-i).timestamp())\n",
    "    host=random.choice(hosts)\n",
    "    source=random.choice(sources)\n",
    "    index=random.choice(indexes)\n",
    "    raw=(datetime.datetime.now()-i).strftime(datetime_format)+\" \"+random.choice(log_lines)\n",
    "    row=time+sep+index+sep+host+sep+source+sep+sourcetype+sep+raw\n",
    "    import_events.write(row+\"\\n\")\n",
    "\n",
    "\n",
    "import_events.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This script generates a basic set of events for importing into splunk \"\"\"\n",
    "\n",
    "datetimes =[]\n",
    "for i in range(1,sample_readings) :\n",
    "    random_seconds = random.randrange(1, date_range_days*seconds_in_day)\n",
    "    datetimes.append(datetime.timedelta(seconds=-random_seconds))\n",
    "\n",
    "datetimes.sort(reverse=True)\n",
    "\n",
    "split_forwarding = open(\"sample/split_forwarding/events.log\",\"w\")\n",
    "\n",
    "for i in datetimes :\n",
    "    split_forwarding.write((datetime.datetime.now()-i).strftime(\"%Y-%m-%d %H:%M:%S\")+\" \"+random.choice(log_lines)+\"\\n\")\n",
    "\n",
    "split_forwarding.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}