{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook generates sample data for the app and writes to the sample directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the global variables for the script\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "\"\"\" \n",
    "For events we have selected a selection of Splunk T shirt sloans. This list was obtained by searching the web, it is not a definitive list and I suspect many were never printed :-)\n",
    "\"\"\"\n",
    "log_lines = open(\"sample/log_lines.txt\",\"r\").read().splitlines()\n",
    "\n",
    "\"\"\"\n",
    "The script generates events randomly over a time range, by default this goes back 5 days and generates a 1000 events each time.\n",
    "\"\"\"\n",
    "date_range_days=2\n",
    "sample_readings=1000\n",
    "\n",
    "# Get an random array of datetime objects going backwards in time, sorted oldest first\n",
    "def get_dates(sample_readings : int, max_days_ago : int) : \n",
    "    datetimes = []\n",
    "    for i in range(0,sample_readings): \n",
    "        random_seconds = random.randrange(1, max_days_ago*24*60*60)\n",
    "        my_timedelta=datetime.timedelta(seconds=-random_seconds, milliseconds=random.randint(0,9999))\n",
    "        my_datetime=datetime.datetime.now()+my_timedelta\n",
    "        datetimes.append(my_datetime)\n",
    "    # Sort the dates into reverse chroniclogical order as they would appear in a log file\n",
    "    datetimes.sort(reverse=False)\n",
    "    return datetimes\n",
    "\n",
    "\"\"\"\n",
    "This generates a list of events where the time stamps switches between 3 different timestamps.\n",
    "\n",
    "This is a common problem in badly designed Splunk instances. People open a TCP port and then fire all sorts of different data in there. \n",
    "\n",
    "Ideally we would create multiple sourcetypes and then assign a TCP port for each sourcetype. However the example shows you how to patch the problem during ingestion.\n",
    "\"\"\"\n",
    "def generate_conflicting_dates(sample_readings : int, max_days_ago : int) :\n",
    "\n",
    "    # our three different date time formats\n",
    "    datetime_format = [\"%Y-%m-%d %H:%M:%S\", \"%H:%M:%S %y-%m-%d\", \"%c\"]\n",
    "\n",
    "    # create out output file\n",
    "    mutliplexed_datetime_formats = open(\"sample/conflicting_dates/mutliplexed_datetime_formats.log\",\"w\")\n",
    "\n",
    "    # iterate through the list of date timesn and write out to disk\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        # select a timeformat at random and use it\n",
    "        time=my_datetime.strftime(random.choice(datetime_format))\n",
    "        # pick a random log line to use\n",
    "        message=random.choice(log_lines)\n",
    "        # write out the log file\n",
    "        mutliplexed_datetime_formats.write(time+\" \"+message+\"\\n\")\n",
    "\n",
    "    # close and flush the file\n",
    "    mutliplexed_datetime_formats.close()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script generates events where the date is embedded in the file name, but the timestamp is per event im the contents of the file.\n",
    "\n",
    "We are going to create a map of dates to times, so that we can itterate through each day, create a file and fill with events for that day\n",
    "\n",
    "To work around any weird rounding errors due to timezones we will generate the day, and the seconds separately\n",
    "\"\"\"\n",
    "def generate_files_for_dates(sample_readings : int, max_days_ago : int) : \n",
    "    # create our map for the date to timings mapping\n",
    "    date_map = {}\n",
    "\n",
    "    # populate our map\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        # get the date component from the datetime object\n",
    "        day=my_datetime.strftime(\"%Y-%m-%d\")\n",
    "        if day not in date_map :\n",
    "            date_map[day] = []\n",
    "        date_map[day].append(my_datetime.strftime(\"%H:%M:%S.%f\"))            \n",
    "\n",
    "    # itterate through all the days and print out the times with a random log message\n",
    "    for my_day in date_map.keys() :\n",
    "        filename=\"sample/compound_date_time/\"+my_day+\".log\"    \n",
    "        # filename for the days events, named after the day \"2020-02-12.log\"\n",
    "        file_for_day = open(filename,\"w\")\n",
    "        for my_time in date_map[my_day] :\n",
    "            # write out the timestamp with a random log message\n",
    "            file_for_day.write(my_time + \" \" + random.choice(log_lines)+ \"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "This script generates a csv with 'useless' columns that we don't want to add into tsidx because they will bloat the size of the bucket.\n",
    "\n",
    "We use pandas to build the CSV file, set headers etc\n",
    "\"\"\"\n",
    "\n",
    "def generate_drop_columns_csv(sample_readings : int, max_days_ago : int)  :\n",
    "    # create a pandas with some column headings describing the contents\n",
    "    useless_columns=pandas.DataFrame(columns=['primary_key', 'primary_value', 'repeated_field', 'random_nonsense', 'long_payload'])\n",
    "\n",
    "    # Create rows and assign values to the columns\n",
    "    for my_date in get_dates(sample_readings, max_days_ago) :\n",
    "        useless_columns=useless_columns.append({'primary_key': my_date, 'primary_value': random.randint(0,999999), 'repeated_field': \"same silly value\", 'random_nonsense' : uuid.uuid4(), 'long_payload' : random.choice(log_lines)}, ignore_index=True)\n",
    "\n",
    "    # write out the CSV file\n",
    "    useless_columns.to_csv('sample/drop_useless_columns/useless_columns.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "\"\"\"\n",
    "This creates log lines with follow an attribute=value pattern using different and no quotes\n",
    "\n",
    "Specify the minimum and maximum number of av pairs per log line\n",
    "\"\"\"\n",
    "def generate_indexed_fields_log(sample_readings : int, max_days_ago : int,  min_values : int, max_values : int) : \n",
    "\n",
    "    # a list of variable names for us to pull from, complete with a type field\n",
    "    variable_names= [('stdev_kbps',float),('average_kbps',float), ('sum_kbps',int), ('label',str), ('name',str), ('group',str), ('value',int)]\n",
    "    # a list of string values for us to pull from when building events\n",
    "    labels = ['no_quotes',\"'single quotes'\",'\"double quotes\"']\n",
    "\n",
    "    # We don't want some n00b specifying more AV pairs than we have in our sample group or we run out!\n",
    "    if (max_values>len(variable_names)\n",
    "        max_values=len(variable_names)\n",
    "\n",
    "    # open our output file\n",
    "    indexed_log = open(\"sample/indexed_log/indexed.log\",\"w\")\n",
    "\n",
    "    # get a selection of date times\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "\n",
    "        # We don't want the same variable printed multiple times, this would result in multivalue fields   \n",
    "        # Copy the our list of possible AV pairs     \n",
    "        my_variables_names=variable_names.copy()\n",
    "        # shuffle that list so they occur in a random order\n",
    "        random.shuffle(my_variables_names)\n",
    "\n",
    "        # we need an message to return, we will put the time stamp at the front\n",
    "        message = my_datetime.strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "        # We will append a random number of AV pairs to the message\n",
    "        for i in range(0,random.randint(min_values,max_values)) :\n",
    "            # pop off the variable name and the type that we are going to use\n",
    "            (variable_name, variable_type) = my_variables_names.pop(1)\n",
    "            # write out the variable name\n",
    "            message = message + \" \" + variable_name + \"=\" \n",
    "            # depending the type select a value\n",
    "            if (variable_type == str) : \n",
    "                message = message + random.choice(labels)\n",
    "            elif (variable_type == float) :\n",
    "                message = message + str(random.random())\n",
    "            elif (variable_type == int) :\n",
    "                message = message + str(random.randint(0,9999))\n",
    "\n",
    "        # write out the log name\n",
    "        indexed_log.write(message+\"\\n\")\n",
    "\n",
    "    # close the file\n",
    "    indexed_log.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This script generates a data set for importing into directly into splunk. We have create sourcetype, source, host, index and then use INGEST_EVAL + REGEX to extract the fields and copy them into the relevant fields. \n",
    "\n",
    "The format aims to replicated the output of the following splunk search:\n",
    "\"\"\"\n",
    "def generate_input_file :\n",
    "\n",
    "    # for this demo we needs some target indexes, sourcestypes, sources and hosts\n",
    "\n",
    "    # these indexes have been created in indexes.conf\n",
    "    indexes=['ingest_pipeline_demo_a', 'ingest_pipeline_demo_b']\n",
    "    # sourcetypes should have only one date format, lets match these together, these don't need to be defined in props.conf as the timestamp is written out in EPOCH\n",
    "    sourcetypes=[('fruit', \"%c\"), ('beef', \"%Y-%m-%d %H:%M:%S\"), ('fish', \"%H:%M:%S %y-%m-%d\"), ('chicken', \"%d %a %Y %H:%M:%S\")]\n",
    "    # a selection of values for source\n",
    "    sources=['sea', 'ground', 'sky', 'tree']\n",
    "    # a selection of values for host\n",
    "    hosts=['farm_shop', 'online', 'super_market', 'market']\n",
    "    # we also need something to separate the data\n",
    "    sep=\"%%%\"\n",
    "\n",
    "    # open the output log file to write data too\n",
    "    import_events = open('sample/import_data/encoded_splunk_events.csv',\"w\")\n",
    "\n",
    "    # get a selection of date times\n",
    "    for my_datetime in get_dates(sample_readings, max_days_ago) :\n",
    "        # pick a sourcetype adn \n",
    "        (sourcetype, datetime_format) = random.choice(sourcetypes)\n",
    "        # get the epoch numeric value for the datetime\n",
    "        time=str(my_datetime.timestamp())\n",
    "        # pick a random host, source and index for the log line\n",
    "        host=random.choice(hosts)\n",
    "        source=random.choice(sources)\n",
    "        index=random.choice(indexes)\n",
    "        # generate the log line prefixed by the timestamp\n",
    "        raw=my_datetime.strftime(datetime_format)+\" \"+random.choice(log_lines)\n",
    "        # write out the line to be written into the import file\n",
    "        import_events.write(time + sep + index + sep + host + sep + source + sep + sourcetype + sep + raw + \"\\n\")\n",
    "\n",
    "    # write to the output file\n",
    "    import_events.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}