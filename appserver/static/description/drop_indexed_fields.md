# Problem statement
Splunk provides a parsing options for reading structured data `INDEXED_EXTRACTIONS` which can primarily be applied to `CSV` and `JSON` data. Both of these options use a parser to extract the structured data and writes attribute value pairs into the `_meta` string to appear as indexed fields. This allows the user to accelarate `_raw` search with `TERM()` or use the SQL like `tstats` command on your data. Doing this can improve performance by more than 40 times.However the disadvance of such an approach is buckets sizes can significantly increase due to the extra entiries in the `tsidx` file. This can be expecially problematic when there is large number of CSV columns as this leads to a correspondingly large number of entitires in your lexicon. In extreme cases enabling `INDEXED_CSV` can more than double your bucket size and can comprise your retention, especially if your sized with the rule of thumb described in the documentation [Estimate your storage requirements](https://docs.splunk.com/Documentation/Splunk/8.0.6/Capacity/Estimateyourstoragerequirements)

## Existing solution
If you have a large number of columns you are often forced to use search time enrichement instead of `INDEXED_EXTRACTIONS` and this impacts your uses cases and performance. You may choose to use a datamodel to create a smaller data set for effcient searching, which in turn is expensive on CPU usage.

### INDEXED_EXTRACTIONS options

    INDEXED_EXTRACTIONS = <CSV|TSV|PSV|W3C|JSON|HEC>
    * The type of file that Splunk software should expect for a given source
    type, and the extraction and/or parsing method that should be used on the file.
    * The following values are valid for 'INDEXED_EXTRACTIONS':
    CSV  - Comma separated value format
    TSV  - Tab-separated value format
    PSV  - pipe ("|")-separated value format
    W3C  - World Wide Web Consortium (W3C) Extended Log File Format
    JSON - JavaScript Object Notation format
    HEC  - Interpret file as a stream of JSON events in the same format
            as the HTTP Event Collector (HEC) input.
    * These settings change the defaults for other settings in this subsection
    to appropriate values, specifically for these formats.
    * The HEC format lets events overried many details on a per-event basis, such
    as the destination index. Use this value to read data which you know to be
    well-formatted and safe to index with little or no processing, such as
    data generated by locally written tools.
    * Default: not set

Source [props.conf](https://docs.splunk.com/Documentation/Splunk/8.0.6/Admin/Propsconf)

## Alternative approach 
With the introduction of `INGEST_EVAL` we can easily remove indexed fields from the `_meta` string. This allows us to keep the most important columns as indexed fields, and then use search time extraction for the less important ones. This hybrid approach allows us to keep performance of indexed fields, while keeping bucket size down.

### Example data
To demonstrate how `INGEST_EVAL` can resolve the issue this application generates some sample data. Notice the column names and the associated values

    time,primary_key,primary_value,repeated_field,random_nonsense,long_payload
    2020-08-30 14:53:17.245855,0x97a5,625362,same silly value,b5c93b49-37e6-4d63-9dfe-447bbed8b128,splunk> IT like you mean it 
    2020-08-30 17:20:19.180806,0xb8f8,172464,same silly value,bc4a273f-9238-4f16-815c-55903f4dfe0d,Splunk> 4TW 
    2020-08-30 23:39:11.662594,0xe1cd,695140,same silly value,94447247-ce80-44a2-a788-3a539fe0ee7d,"Splunk> see the forest, and the trees "
    2020-08-31 00:23:52.489584,0xeb4a,374488,same silly value,1ec2f016-cc9b-4c54-ad30-f187f47dafda,splunk> this way: Run-D.M.C. 
    2020-08-31 00:38:50.033850,0x8063,455774,same silly value,469fc7c9-b9c6-4582-8afe-e222fcad94fe,Splunk> All batbelt. No tights. 
    2020-08-31 07:58:40.252550,0xd538,884285,same silly value,074e9326-4c1c-4d39-a0e2-25fbf7c59a0c,splunk> Caught me on the server - Wasn't me. 
    2020-08-31 08:34:48.438904,0xc5b0,30242,same silly value,af7d0326-2edb-40be-bb9e-25989971ad61,splunk> because ninjas are too busy 
    2020-08-31 12:53:47.761629,0xb179,731284,same silly value,b717906c-262c-4375-bdbb-4b49b9a9c1bd,splunk> Chasing tail since 2003 
    2020-08-31 17:11:42.791025,0xaf46,808337,same silly value,f376d805-c536-4961-9658-c941ed682177,splunk> We line break for regular expressions 

### How we will handle the columns

* `time` - Contains the field containing the time stamp, we need this for _time
* `primary_key` - Contains the primary field, this is a perfect field for an indexed field
* `primary_value` - Contains the primary vaoue, this is a perfect field for an indexed field
* `repeated_field` - The same value every time, no value to this field at all, we can just drop it
* `random_nonsense` - Some random nonsense with high cardinality, this would bloat tsidx
* `long_payload` - This is some random log messages, this would make an awful indexed field

### Steps

1. Set props to load data with `INDEXED_EXTRACTIONS=CSV`
    1. Set the time field to in the `time` column
    1. Set the format for the time field
1. Create use `INGEST_EVAL` to assign `null()` using the overite assignment operator `:=` to any index field we wanted remove
    1. Drop, `repeated_field`, `random_nonsense` and `long_payload`
1. Create a search time lookup (i.e. regex) to extract the fields we removed as indexed fields
    1. Use regex to extract `random_nonsense` and `long_payload` at search time

### props.conf

    # this is an example for removing unwanted indexed fields from a CSV file
    [reduce_columns]
    # The timestamp is found in the time column, we need to remove this column
    TIMESTAMP_FIELDS=time
    INDEXED_EXTRACTIONS = CSV
    # These transforms remove the useless columns, and also the useless time fields
    TRANSFORMS-drop_fields = drop_useless_columns, drop_useless_time_fields
    # Lift the removed columns via search time exraction (only works in the app)
    EXTRACT-removed-columns= [^,]+,[^,]+,[^,]+,(?<random_nonsense>[^,]+),(?<long_payload>[^,]+)

### transforms.conf

    # To drop a field from _meta we need to overwrite any previous value using the := assignment option
    [drop_useless_columns]
    INGEST_EVAL= time:=null(), repeated_field:=null(), random_nonsense:=null(), long_payload:=null()

    # To drop a field from _meta we need to overwrite any previous value using the := assignment option
    [drop_useless_time_fields]
    INGEST_EVAL= timestartpos:=null(), timeendpos:=null(), date_second:=null(), date_hour:=null(), date_minute:=null(), date_year:=null(), date_month:=null(), date_mday:=null(),  date_wday:=null(), date_zone:=null()

